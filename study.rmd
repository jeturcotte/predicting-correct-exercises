---
title: "Predicting Exercise Correctness by Accelerometer Device"
author: "J.E. Turcotte"
date: "June 11, 2016"
output: html_document
---

```{r loading and labelling the data, message=F, warning=F, echo=F }
library(ggplot2)
library(grid)
library(gridExtra)
library(knitr)
library(pander)
library(car)
library(caret)

set.seed(7041)

grid_arrange_shared_legend <- function(...) {
    plots <- list(...)
    g <- ggplotGrob(plots[[1]] + theme(legend.position="bottom"))$grobs
    legend <- g[[which(sapply(g, function(x) x$name) == "guide-box")]]
    lheight <- sum(legend$height)
    grid.arrange(
        do.call(arrangeGrob, lapply(plots, function(x)
            x + theme(legend.position="none"))),
        legend,
        ncol = 1,
        heights = unit.c(unit(1, "npc") - lheight, lheight)
    )
}
```
```{r get the data, echo=F}
trn <- read.csv("data/pml-training.csv", na.strings=c("","#DIV/0!","NA"), stringsAsFactors=F )
trn$classe <- as.factor(trn$classe)
```

## Executive Summary

Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

## Assumptions

This predictive model assumes that...

* We do not need to know the individual who is doing the exercise, insofar as doing an exercise correctly or incorrectly in a number of predictable ways is common to all people.
* It might be of interest to treat each variable as a time sequence, given the timestamps available, but that we should still be able to predict future movement classes without this.

## Data Examination and Repair

```{r normalizing test data, echo=F }

# find any columns that have essentially zero variation
cull <- nearZeroVar( trn, saveMetrics=T )
cull.alpha <- rownames( cull[cull$zeroVar==T,] )

# create a means for measuring ratio of any given variable's na vs. useful values
na.ratio <- function(vn) { sum( is.na(vn) / length(vn) ) }
cull.beta <- names( which( sapply( trn, na.ratio ) > 0.9 ) )

# further columns that we don't want to know about and could cloud our model
cull.gamma <- c('X','user_name','raw_timestamp_part_1','raw_timestamp_part_2','cvtd_timestamp','num_window','new_window')
```

Given the large number of variables available, a quick look through **Fig.1** -- *a table of variation and uniqueness within the training data set* -- suggests that we can afford to cull quite a few.  In **Fig.2** we see all that remain after selecting out the union of *`r length(cull.alpha)`* zero variation variables, *`r length(cull.beta)`* variables that are more than 90 'na', and *`r length(cull.gamma)`*, like *user_name* against which we specifically do NOT want to predict, or are otherwise not important and might confuse our results if left intact.

Interestingly enough, the union of the first and second group, with *`r length( unique( c( cull.alpha, cull.beta ) ) )`* variables, says that all the no-variation data fit snugly within the larger group of variables that were ruled almost entire by NA values.  Less the unnecessary columns, that leaves *`r length( colnames( trn ) )`* variates to fit.

## Partitioning of Training and Test Data

```{r apply alterations to new data frames and make subselections, eval=T, echo=F }
trn <- trn[, !names(trn) %in% unique( c( cull.alpha, cull.beta, cull.gamma ) ) ]
subselection <- createDataPartition( trn$classe, p=0.9 )
# note that, as of R 3.3.0, it appears that $Resample1 must be included
training <- trn[subselection$Resample1,]
testing <- trn[-subselection$Resample1,]
```

For the purposes of this predictive model production, we will be splitting our initial training data two ways. *`r length(training$classe)`* remain in the training set while *`r length(testing$classe)`* will become our final in-study test.

## Examining Possible Models

```{r set up common kfold controls, eval=T, echo=F }
controls <- trainControl( method="cv", number=5 )
```
```{r try a simple k-fold rpart, cache=T, echo=F }
cv_rpart_model <- train( classe ~ ., training, method="rpart", trControl=controls )
confused_cv_rpart <- confusionMatrix( testing$classe, predict( cv_rpart_model, newdata=testing ) )
plot_cv_rpart <- ggplot( as.data.frame( as.table( confused_cv_rpart$table ) ) )
plot_cv_rpart <- plot_cv_rpart + geom_tile( aes( x=Reference, y=Prediction, fill=Freq) )
plot_cv_rpart <- plot_cv_rpart + scale_fill_distiller( palette="Spectral" )
plot_cv_rpart <- plot_cv_rpart + geom_text( aes(x=Reference, y=Prediction, label=Freq ) )
```
```{r and now a fully fitted random forest, cache=T, echo=F }
rf_model <- train( classe ~ ., training, method="rf", trControl=controls )
confused_rf <- confusionMatrix( testing$classe, predict( rf_model, newdata=testing ) )
plot_rf <- ggplot( as.data.frame( as.table( confused_rf$table ) ) )
plot_rf <- plot_rf + geom_tile( aes( x=Reference, y=Prediction, fill=Freq) )
plot_rf <- plot_rf + scale_fill_distiller( palette="Spectral" )
plot_rf <- plot_rf + geom_text( aes(x=Reference, y=Prediction, label=Freq ) )
```
```{r combine the plots, eval=T, echo=F, fig.align='right', fig.width=4, fig.height=5}
grid_arrange_shared_legend( plot_cv_rpart, plot_rf )
```
Note that, in both of the following cases, we've already effectively disqualified a large number of potential predictors, so we begin (and find that we can end) with the most basic of model formalae.

First up, we give a simpler model, an *rpart*, a try, just to see how effective it might be.  With a 5 segment k-fold cross validation applied across all variables (***classe ~ .***), we ultimately get a poor result at a mere `r sprintf("%0.2f",confused_cv_rpart$overall[1]*100)`% accuracy rate when applied to our testing set (*top plot*.)  It appears that we are able to fairly easily identify *class A*, aka 'correct', exercise movements, but the rest are all over the place.  In fact, very few *class E* events were predicted, and none among *class E*.

In the lower plot, the confusion matrix for a **random forest** model, (again, ***classe ~ .***), also enduring a 5 segment k-fold cross validation process before being applied to the isolated test segment.  As can be seen in the plot, this method proved extremely accurate, given that if we'd significantly overfit our model, the rate should be a great deal lower than the `r sprintf("%0.2f",confused_rf$overall[1]*100)`% we got.

## Concluding with a Simpler Model

```{r whittle it down a bit, eval=T, echo=F }
imp <- varImp(cv_rpart_model)$importance
imp$predictor <- row.names(imp)
new_predictors <- imp[imp$Overall > 33,]$predictor
```
```{r one last random forest refuge, eval=T, echo=F, cache=T }
simpler_formula <- as.formula(paste("classe ~ . -", paste(new_predictors,collapse=" - ")))
simpler_model <- train( simpler_formula, training, method="rf", trControl=controls )
simply_confused <- confusionMatrix( testing$classe, predict( simpler_model, newdata=testing ) )
plot_simpler <- ggplot( as.data.frame( as.table( simply_confused$table ) ) )
plot_simpler <- plot_simpler + geom_tile( aes( x=Reference, y=Prediction, fill=Freq) )
plot_simpler <- plot_simpler + scale_fill_distiller( palette="Spectral" )
plot_simpler <- plot_simpler + geom_text( aes(x=Reference, y=Prediction, label=Freq ) )
```
```{r display final confusion matrix, eval=T, echo=F, fig.align='right', fig.width=4, fig.height=5 }
plot_simpler
```
Given the aforementioned `r sprintf("%0.2f",confused_rf$overall[1]*100)`% accuracy rate earned against the test data, there's room now to explore how many of the predictors are actually useful.  This, we see in **Fig.3**, in the appendix.  It turns out that *`r sum(imp$importance > 0)`* of the predictors, as a result of the **caret** package random forest algorithms, are considered useful... something we trust, given the model's accuracy against the test data.

As one can see in the right-hand plot, the net result, with an accuracy of ***`r sprintf("%0.2f",simply_confused$overall[1]*100)`%***, the following *`r length(new_predictors)`* predictors were enough to provide a very very accuracte prediction model.

```{r display predictors, eval=T, echo=F }
pander(new_predictors)
```

## Source

Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6. 
Cited by 2 (Google Scholar)

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz4Bz9LVKWs

## Appendices

#### Fig.1; Near Zero Variation

A quick look at the nature of the various columns we must examine.  Quite a few either have zero or near zero variation, rendering them expendible for the purposes of this data exploration.

```{r near zero variation detection, echo=F, eval=T}
pander(nearZeroVar(trn,saveMetrics=T))
```

#### Fig.2; After Culling Needless Variables

After studying the nature of the variables a few times, this study has whittled down those that will be considered for the prediction model down to the following.

```{r display high amounts of na, echo=F, eval=T}
pander( summary( trn[, !names(trn) %in% unique( c( cull.alpha, cull.beta, cull.gamma ) ) ] ) )
```

#### Fig.3; The Most Important Variables

A quick look at at the predictors used in the very successful *random forest* model, below.
```{r display the most important variables, echo=F, eval=T}
pander( varImp(cv_rpart_model) )
```